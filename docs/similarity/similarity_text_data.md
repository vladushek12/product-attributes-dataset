# Методы сравнения близости текстов

## Сравнение текстового представления

#### **Расстояние Левенштейна**

Расстояние Левенштейна (редакционное расстояние, дистанция редактирования) — метрика, измеряющая по модулю разность между двумя последовательностями символов. Она определяется как минимальное количество односимвольных операций (а именно вставки, удаления, замены), необходимых для превращения одной последовательности символов в другую. В общем случае, операциям, используемым в этом преобразовании, можно назначить разные цены. Широко используется в теории информации и компьютерной лингвистике.

Расстояние Левенштейна широко используется для:
* нахождения расхождений текстовых файлов;
* исправления ошибок в слове (в поисковых системах, базах данных, при вводе текста, при автоматическом распознавании отсканированного текста или речи);
* для сравнения генов, хромасом и белков в биоинформатике;
* аналог рассматриваемой метрики был использован Филипповым Д. В. в ядерной физике при построении модели низкоэнергетических ядерных реакций расстояние между двумя ансамблями ядер определялось через количество элементарных преобразований, последовательность которых переводила один ансамбль в другой.

$$
d_{Lev}(A, B) = 
\begin{cases} 
|A| & \text{если } |B| = 0, \\
|B| & \text{если } |A| = 0, \\
\min \begin{cases}
d_{Lev}(A[1:], B) + 1 \\
d_{Lev}(A, B[1:]) + 1 \\
d_{Lev}(A[1:], B[1:]) + \mathbf{1}_{(A[0] \neq B[0])}
\end{cases} & \text{иначе.}
\end{cases}
$$

где $ A $ и $ B $ — строки символов, а $\mathbf{1}_{(A[0] \neq B[0])}$ — индикаторная функция, равная 1, если символы не совпадают, и 0 в противном случае.

#### **Расстояние Дамерау-Левенштейна**

Расстояние Дамерау — Левенштейна (названо в честь учёных Фредерика Дамерау и Владимира Левенштейна) — это мера разницы двух строк символов, определяемая как минимальное количество операций вставки, удаления, замены и транспозиции (перестановки двух соседних символов), необходимых для перевода одной строки в другую. Является модификацией расстояния Левенштейна: к операциям вставки, удаления и замены символов, определённых в расстоянии Левенштейна добавлена операция транспозиции (перестановки) символов.

$$
d_{DL}(A, B) = \min \begin{cases}
d_{Lev}(A, B), \\
d_{DL}(A[2:], B[2:]) + 1 & \text{если } A[0] = B[1] \text{ и } A[1] = B[0].
\end{cases}
$$

#### **Расстояние Хэмминга**

Расстоя́ние Хэ́мминга (кодовое расстояние) — число позиций, в которых соответствующие символы двух слов одинаковой длины различны. В более общем случае расстояние Хэмминга применяется для строк одинаковой длины любых q-ичных алфавитов и служит метрикой различия (функцией, определяющей расстояние в метрическом пространстве) объектов одинаковой размерности.

**Формула:**
$$
d_H(A, B) = \sum_{i=1}^{|A|} \mathbf{1}_{(A[i] \neq B[i])},
$$
где |A| = |B|.

#### **Коэффициент Жаккара**
В области информатики и статистики сходство Джаро — Винклера представляет собой меру схожести строк для измерения расстояния между двумя последовательностями символов. Это вариант, который в 1999 году предложил Уильям Э. Винклер (William E. Winkler) на основе расстояния Джаро (1989, Мэтью А. Джаро, Matthew A. Jaro). Неформально, расстояние Джаро между двумя словами — это минимальное число односимвольных преобразований, которое необходимо для того, чтобы изменить одно слово в другое.

**Формула:**

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}.
$$

#### **Метрика Tversky**

Обобщает коэффициент Жаккара, позволяя учитывать веса для различных типов совпадений.  
**Формула:**
$$
Tversky(A, B; \alpha, \beta) = \frac{|A \cap B|}{|A \cap B| + \alpha |A \setminus B| + \beta |B \setminus A|}.
$$

#### **Дивергенция Йенсена — Шеннона**

Мера разницы между двумя распределениями вероятностей, часто используется для сравнения текстов через их языковые модели.

**Формула:**

$$
D_{JS}(P, Q) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(Q \| M),
$$
где $ M = \frac{1}{2}(P + Q) $, а $ D_{KL} $ — дивергенция Кульбака — Лейблера.

#### **Longest Common Subsequence**

Задача нахождения наибольшей общей подпоследовательности (англ. longest common subsequence, LCS) — задача поиска последовательности, которая является подпоследовательностью нескольких последовательностей (обычно двух). Часто задача определяется как поиск всех наибольших подпоследовательностей. Это классическая задача информатики, которая имеет приложения, в частности, в задаче сравнения текстовых файлов (утилита diff), а также в биоинформатике.

$$
LCS(A, B) = 
\begin{cases} 
0 & \text{если } |A| = 0 \text{ или } |B| = 0, \\
LCS(A[:-1], B[:-1]) + 1 & \text{если } A[-1] = B[-1], \\
\max(LCS(A[:-1], B), LCS(A, B[:-1])) & \text{иначе.}
\end{cases}
$$

#### **Перекрытие терминов**
Подсчитывает общее число совпадающих слов или фраз.  

**Формула:**

$$
Overlap(A, B) = \sum_{t \in A \cap B} \min(\text{freq}(t, A), \text{freq}(t, B)),
$$
где $ \text{freq}(t, X) $ — частота слова $ t $ в тексте $ X $.

## Сравнение векторных представлений 

### Способы представления текстов в вектор

#### **IDF**

IDF — это статистический показатель, который оценивает важность слова в коллекции документов. Чем реже слово встречается в наборе документов, тем выше его IDF-значение, что делает его более значимым для анализа. 

IDF используется вместе с TF (Term Frequency) для создания метрики TF-IDF.

$$
\text{IDF}(t) = \log \left( \frac{N}{1 + |\{d \in D : t \in d\}|} \right),
$$
где $ N $ — общее количество документов, $ D $ — коллекция документов, $ t $ — термин.

#### **TF-IDF**

TF-IDF — это комбинация двух метрик: частоты термина (TF ) и обратной частоты документа (IDF ). Он помогает определить важность слова в конкретном документе относительно всей коллекции.

TF-IDF особенно полезен для задач информационного поиска, классификации текстов и анализа ключевых слов 

$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \cdot \text{IDF}(t),
$$
где $ \text{TF}(t, d) $ — частота термина $ t $ в документе $ d $.

#### **Word2Vec**

Word2Vec — это модель машинного обучения, которая преобразует слова в векторы фиксированной длины, сохраняя при этом семантические отношения между ними. Например, вектор "король" близок к вектору "принц", а вектор "женщина" близок к вектору "девушка". Word2Vec работает двумя основными способами:

* CBOW (Continuous Bag of Words) : предсказывает слово по его контексту.
* Skip-gram : предсказывает контекст по слову.
  
Word2Vec создает плотные векторные представления слов, которые можно использовать для анализа текстов или как входные данные для других моделей

#### **BoW**

Метод, в котором текст представляется как мешок слов без учета порядка. Каждому слову присваивается вес, равный количеству его появлений в тексте. BoW простой, но **плохо работает с длинными текстами из-за высокой размерности**.

$$
\text{BoW}(d) = [f_1, f_2, \dots, f_V],
$$
где $ f_i $ — частота $ i $-го слова в словаре размера $ V $.

#### **Эмбеддинги**

Большие языковые модели (LLM), такие как BERT , GPT , или T5 , используются для генерации эмбеддингов текста. Эти модели обучены на огромных корпусах данных и могут улавливать глубокие семантические связи между словами и предложениями. Процесс получения эмбеддингов из LLM включает следующие шаги:

Токенизация : Разделение текста на токены (слова или подслова).
Встраивание токенов : Каждый токен преобразуется в векторное представление.
Контекстуализация : Модель учитывает контекст, чтобы адаптировать векторы токенов.
Объединение : Векторы всех токенов объединяются в единый вектор для всего текста (например, путем усреднения).
Эмбеддинги из LLM обычно более качественные, чем Word2Vec, так как они учитывают контекст и взаимосвязь между словами.

### Реализации сравнения

#### **Евклидово расстояние**

Евклидово расстояние представляет собой геометрическое расстояние между точками в многомерном пространстве.

$$
d_E(v_1, v_2) = \sqrt{\sum_{i=1}^n (v_{1,i} - v_{2,i})^2}.
$$

#### **Манхэттенское расстояние**

Это сумма абсолютных разностей соответствующих компонент двух векторов. Манхэттенское расстояние иногда используется вместо евклидова, особенно когда нужно учитывать каждую координату независимо.

$$
d_M(v_1, v_2) = \sum_{i=1}^n |v_{1,i} - v_{2,i}|.
$$

#### **Косинусное расстояние** 

Косинусное расстояние измеряет угловое сходство между векторными представлениями текстов. Оно определяется как угол между двумя векторами в многомерном пространстве и нормализованное скалярное произведение этих векторов. Косинусное расстояние не зависит от длины векторов, что делает его особенно полезным при работе с текстами разной длины.

$$
d_C(v_1, v_2) = 1 - \cos(\theta) = 1 - \frac{v_1 \cdot v_2}{\|v_1\| \|v_2\|}.
$$

#### **Word Mover's Distance**

Word Mover's Distance (WMD) — это метод, который позволяет оценивать «расстояние» между двумя документами даже тогда, когда они не имеют общих слов, но их смысл может быть близок благодаря семантической связи между словами 7. WMD основывается на эмбеддингах слов (например, Word2Vec) и вычисляет минимальную "стоимость" перемещения массы из одного множества векторов (текста) в другое, что эквивалентно решению задачи оптимального транспортирования. Это делает WMD особенно полезным для анализа схожести длинных текстов или документов, где важно учитывать контекст и семантику слов.

$$
WMD(D_1, D_2) = \min_{\mathbf{f}} \sum_{w_i \in D_1} \sum_{w_j \in D_2} f_{ij} \cdot d(w_i, w_j),
$$
где $ f_{ij} $ — поток между словами $ w_i $ и $ w_j $, а $ d(w_i, w_j) $ — расстояние между их эмбеддингами.

![](Картинки\sphx_glr_run_wmd_001.png)

## Источники

https://cda.vavt.ru/articles/article_62.pdf \
https://habr.com/en/articles/852046/ \
https://en.wikipedia.org/wiki/String_metric \
https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html\
https://www.newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python
